# ðŸš– NYC Taxi Big Data Analytics

## ðŸ“‹ DescripciÃ³n del Proyecto

Este proyecto es una soluciÃ³n **End-to-End de Big Data** diseÃ±ada para procesar, analizar y visualizar millones de registros de viajes de taxis de Nueva York. Utiliza un stack moderno basado en contenedores para garantizar portabilidad y escalabilidad.

El sistema ingesta datos crudos (Parquet), los procesa con **Apache Spark** en un cluster **Hadoop**, expone los resultados a travÃ©s de una **API REST (Node.js)** y los visualiza en un **Dashboard Interactivo (Next.js)**.

---

## ðŸ—ï¸ Arquitectura del Sistema

El siguiente diagrama ilustra el flujo de datos desde la ingesta hasta la visualizaciÃ³n:

```mermaid
graph TD
    subgraph "1. Ingesta de Datos"
        Local[Datos Locales (.parquet)] -->|load_to_hdfs.py| HDFS_Raw[HDFS: /raw/taxi-trips]
    end

    subgraph "2. Procesamiento (Spark)"
        HDFS_Raw -->|clean_data.py| Spark[Apache Spark Cluster]
        Spark -->|Limpieza| HDFS_Clean[HDFS: /processed/cleaned-trips]
        HDFS_Clean -->|analytics_advanced.py| HDFS_Analytics[HDFS: /analytics/v2/*.json]
    end

    subgraph "3. Capa de Servicios"
        HDFS_Analytics -->|WebHDFS| API[API REST (Node.js/Express)]
        API -->|JSON Response| Frontend[Frontend (Next.js/React)]
    end

    subgraph "4. VisualizaciÃ³n"
        Frontend --> User[Usuario Final (Dashboard)]
    end

    style Spark fill:#f9f,stroke:#333,stroke-width:2px
    style HDFS_Analytics fill:#bbf,stroke:#333,stroke-width:2px
    style API fill:#dfd,stroke:#333,stroke-width:2px
```

---

## ðŸ“‚ Organigrama de Directorios

AquÃ­ tienes un mapa de alto nivel para navegar por el cÃ³digo fuente:

```bash
nyc-taxi-bigdata/
â”œâ”€â”€ api/                  # ðŸŸ¢ CÃ³digo Fuente del Backend (Node.js)
â”‚   â”œâ”€â”€ src/              # LÃ³gica de rutas de la API (v1, v2)
â”‚   â””â”€â”€ package.json      # Dependencias del backend
â”œâ”€â”€ fronted/              # ðŸŽ¨ CÃ³digo Fuente del Frontend (Next.js)
â”‚   â”œâ”€â”€ app/              # PÃ¡ginas y rutas (App Router)
â”‚   â”œâ”€â”€ components/       # GrÃ¡ficos (Recharts) y UI (Shadcn)
â”‚   â””â”€â”€ lib/              # Tipos y funciones de utilidad
â”œâ”€â”€ spark-jobs/           # âš¡ Scripts de Procesamiento Python/Spark
â”‚   â”œâ”€â”€ load_to_hdfs.py   # Carga de datos
â”‚   â”œâ”€â”€ clean_data.py     # Limpieza
â”‚   â””â”€â”€ analytics_*.py    # GeneraciÃ³n de mÃ©tricas
â”œâ”€â”€ scripts/              # ðŸ¤– Scripts de AutomatizaciÃ³n y Utilidad
â”‚   â”œâ”€â”€ init.sh           # InicializaciÃ³n interna de contenedores
â”‚   â”œâ”€â”€ verify_env.bat    # Script maestro de despliegue en Windows
â”‚   â””â”€â”€ check_api.bat     # Verificador de estado de API
â”œâ”€â”€ data/                 # ðŸ“¦ Datos Locales (Mapeados a Docker)
â”‚   â””â”€â”€ raw/              # Coloca aquÃ­ tus carpetas de aÃ±os (ej. 2024, 2025)
â”œâ”€â”€ docs/                 # ðŸ“š DocumentaciÃ³n Centralizada
â”‚   â””â”€â”€ index.md          # <-- PUNTO DE PARTIDA DOCUMENTAL
â”œâ”€â”€ docker-compose.yml    # ðŸ³ OrquestaciÃ³n de contenedores
â””â”€â”€ Makefile              # Atajos de comandos
```

---

## ðŸ“š DocumentaciÃ³n

Toda la documentaciÃ³n detallada se encuentra en el directorio `docs/`. Para una guÃ­a ordenada por rol del equipo, consulta:

ðŸ‘‰ **[Ãndice de DocumentaciÃ³n (docs/index.md)](docs/index.md)**

### Acceso RÃ¡pido por Rol:

*   **Ingenieros de Datos / DevOps**:
    *   [GuÃ­a de Despliegue y Comandos](docs/despliegue_comandos.md) (CÃ³mo iniciar todo)
    *   [Scripts de AutomatizaciÃ³n](docs/scripts_automatizacion.md)
*   **Desarrolladores Backend**:
    *   [DocumentaciÃ³n de API](docs/despliegue_comandos.md)
*   **Desarrolladores Frontend**:
    *   [DocumentaciÃ³n de Frontend](docs/frontend_documentacion.md)

---

## ðŸš€ Inicio RÃ¡pido (Quick Start)

Para levantar todo el entorno desde cero en Windows:

1.  AsegÃºrate de tener **Docker Desktop** corriendo.
2.  Abre una terminal en la raÃ­z del proyecto.
3.  Ejecuta:

    ```powershell
    docker-compose up -d --build
    .\scripts\verify_env.bat
    ```

4.  Espera unos minutos a que Spark procese los datos.
5.  Abre tu navegador en:
    *   **Dashboard**: [http://localhost:3001](http://localhost:3001)
    *   **Hadoop UI**: [http://localhost:9870](http://localhost:9870)
    *   **Spark Master**: [http://localhost:8080](http://localhost:8080)
